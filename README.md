# FluidMoE - Complete Custom Layer for Megatron-LM

## ç®€ä»‹

**FluidMoE** æ˜¯ Megatron-LM çš„å®Œå…¨è‡ªå®šä¹‰å±‚å®ç°ï¼Œæä¾›è®¡ç®—é€šä¿¡é‡å ä¼˜åŒ–ï¼Œæ”¯æŒå‰å‘å’Œåå‘ä¼˜åŒ–ã€‚

**æ ¸å¿ƒç‰¹æ€§**:
- ğŸš€ **å®Œå…¨è‡ªå®šä¹‰å±‚** - FluidSelfAttention å’Œ FluidMoELayer
- âš¡ **è®¡ç®—é€šä¿¡é‡å ** - dW è®¡ç®—ä¸ AllToAll é€šä¿¡è‡ªåŠ¨é‡å 
- ğŸ¯ **æ— éœ€ Patch** - ç›´æ¥è°ƒç”¨ Fluid AllToAllï¼Œæ— å…¨å±€å‡½æ•°æ±¡æŸ“
- ğŸ”§ **å®Œå…¨æ§åˆ¶** - å¯è‡ªå®šä¹‰å‰å‘è®¡ç®—é€»è¾‘ï¼Œæ”¯æŒå‰å‘ä¼˜åŒ–
- ğŸ“¦ **å…¼å®¹ Megatron** - ä½¿ç”¨ Layer Spec æœºåˆ¶ï¼Œä¸ Megatron-LM æ— ç¼é›†æˆ

## ğŸš€ 5 åˆ†é’Ÿå¿«é€Ÿå¼€å§‹

```python
from megatron.core.transformer import TransformerConfig
from megatron.core import GPTModel
from fluid import get_fluid_moe_layer_spec

# 1. åˆ›å»ºé…ç½®ï¼ˆMoE + SP + EPï¼‰
config = TransformerConfig(
    num_layers=32,
    hidden_size=4096,
    num_moe_experts=8,
    moe_router_topk=2,
    context_parallel_size=4,           # SP for attention
    expert_model_parallel_size=2,      # EP for MoE
    sequence_parallel=True,
)

# 2. è·å– Fluid layer spec (æ— éœ€ patch!)
layer_spec = get_fluid_moe_layer_spec()

# 3. åˆ›å»ºæ¨¡å‹ï¼ˆFluid ä¼˜åŒ–è‡ªåŠ¨å¯ç”¨ï¼ï¼‰
model = GPTModel(config, transformer_layer_spec=layer_spec)
```

å°±è¿™ä¹ˆç®€å•ï¼æŸ¥çœ‹ [QUICKSTART.md](QUICKSTART.md) äº†è§£æ›´å¤šã€‚

## æ¶æ„è®¾è®¡

```
FluidMoE = å®Œå…¨è‡ªå®šä¹‰å±‚ + æ—  Patch + è®¡ç®—é€šä¿¡é‡å 

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ‚¨çš„ Megatron è®­ç»ƒè„šæœ¬                              â”‚
â”‚  - æ ‡å‡†æ¨¡å‹å®šä¹‰                                      â”‚
â”‚  - æ ‡å‡†è®­ç»ƒå¾ªç¯                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ get_fluid_moe_layer_spec()
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FluidMoE è‡ªå®šä¹‰å±‚ (æ— éœ€ Patch)                      â”‚
â”‚  â”œâ”€ FluidSelfAttention                              â”‚
â”‚  â”‚   - ç›´æ¥è°ƒç”¨ fluid_all_to_all_sp2hp/hp2sp        â”‚
â”‚  â”‚   - æ”¯æŒè‡ªå®šä¹‰å‰å‘è®¡ç®—                            â”‚
â”‚  â”‚   - FluidColumnParallelLinear (dW scheduling)    â”‚
â”‚  â”‚   - FluidRowParallelLinear (dW scheduling)       â”‚
â”‚  â”œâ”€ FluidMoELayer                                   â”‚
â”‚  â”‚   - FluidTokenDispatcher (ç›´æ¥è°ƒç”¨ fluid_all_to_all) â”‚
â”‚  â”‚   - æ”¯æŒè‡ªå®šä¹‰å‰å‘è®¡ç®—                            â”‚
â”‚  â”‚   - FluidGroupedMLP (dW scheduling)              â”‚
â”‚  â””â”€ BackwardScheduler                               â”‚
â”‚      - dW ä»»åŠ¡é˜Ÿåˆ—                                   â”‚
â”‚      - AllToAll åå‘æ—¶æ‰§è¡Œ dW                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ ä½¿ç”¨åŸºç¡€è®¾æ–½
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Megatron-LM                                        â”‚
â”‚  - TransformerLayer                                 â”‚
â”‚  - Router, DotProductAttention                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ç›®å½•ç»“æ„

```
Fluid/
â”œâ”€â”€ fluid/                          # æ ¸å¿ƒä»£ç 
â”‚   â”œâ”€â”€ __init__.py                # æ¨¡å—å…¥å£ (v0.5.0)
â”‚   â”œâ”€â”€ scheduler.py               # dW è°ƒåº¦å™¨
â”‚   â”œâ”€â”€ communication.py           # Fluid AllToAll åŸè¯­
â”‚   â”œâ”€â”€ attention_module.py        # FluidSelfAttention (å®Œå…¨è‡ªå®šä¹‰)
â”‚   â”œâ”€â”€ moe_module.py              # FluidMoELayer + FluidTokenDispatcher
â”‚   â”œâ”€â”€ attention_layers.py        # Fluid çº¿æ€§å±‚ (dW scheduling)
â”‚   â”œâ”€â”€ moe_layers.py              # FluidGroupedMLP (dW scheduling)
â”‚   â””â”€â”€ megatron_layers.py         # Layer Spec é›†æˆ
â”‚
â”œâ”€â”€ examples/                       # ä½¿ç”¨ç¤ºä¾‹
â”‚   â”œâ”€â”€ pretrain_moe.py            # é€šç”¨ MoE è®­ç»ƒ
â”‚   â””â”€â”€ run_mixtral_8x7b.sh       # Mixtral-8x7B è®­ç»ƒè„šæœ¬
â”‚
â”œâ”€â”€ QUICKSTART.md                  # å¿«é€Ÿå…¥é—¨ (æ¨è!)
â”œâ”€â”€ MAINTENANCE_GUIDE.md           # ç»´æŠ¤æŒ‡å—
â”œâ”€â”€ CORE_CONCEPT.md                # æ ¸å¿ƒæ¦‚å¿µ
â””â”€â”€ README.md                      # æœ¬æ–‡æ¡£
```

## æ ¸å¿ƒç»„ä»¶

### 1. FluidSelfAttention (å®Œå…¨è‡ªå®šä¹‰æ³¨æ„åŠ›å±‚)

```python
class FluidSelfAttention(MegatronModule):
    """
    å®Œå…¨è‡ªå®šä¹‰çš„ Self-Attention å±‚
    - å†…éƒ¨ç›´æ¥è°ƒç”¨ fluid_all_to_all_sp2hp/hp2sp (æ— éœ€ patch)
    - æ”¯æŒè‡ªå®šä¹‰å‰å‘è®¡ç®—é€»è¾‘
    - ä½¿ç”¨ FluidColumnParallelLinear/FluidRowParallelLinear
    """

    def forward(self, hidden_states, attention_mask):
        # 1. QKV æŠ•å½±
        mixed_qkv, _ = self.linear_qkv(hidden_states)
        query, key, value = self._split_qkv(mixed_qkv)

        # 2. AllToAll sp2hp (ç›´æ¥è°ƒç”¨,æ— éœ€ patch!)
        if self.cp_size > 1:
            query = fluid_all_to_all_sp2hp(query, group=self.cp_group)
            key = fluid_all_to_all_sp2hp(key, group=self.cp_group)
            value = fluid_all_to_all_sp2hp(value, group=self.cp_group)

        # 3. æ³¨æ„åŠ›è®¡ç®—
        context = self.core_attention(query, key, value, attention_mask)

        # 4. AllToAll hp2sp (ç›´æ¥è°ƒç”¨)
        if self.cp_size > 1:
            context = fluid_all_to_all_hp2sp(context, group=self.cp_group)

        # 5. è¾“å‡ºæŠ•å½±
        output, bias = self.linear_proj(context)
        return output, bias
```

### 2. FluidMoELayer (å®Œå…¨è‡ªå®šä¹‰ MoE å±‚)

```python
class FluidMoELayer(MegatronModule):
    """
    å®Œå…¨è‡ªå®šä¹‰çš„ MoE å±‚
    - å†…éƒ¨ä½¿ç”¨ FluidTokenDispatcher
    - FluidTokenDispatcher ç›´æ¥è°ƒç”¨ fluid_all_to_all (æ— éœ€ patch)
    - æ”¯æŒè‡ªå®šä¹‰å‰å‘è®¡ç®—é€»è¾‘
    """

    def forward(self, hidden_states):
        # 1. Router
        scores, indices = self.router(hidden_states)

        # 2. Token Dispatch (FluidTokenDispatcher ç›´æ¥è°ƒç”¨ fluid_all_to_all)
        dispatched_input, tokens_per_expert, probs = \
            self.token_dispatcher.dispatch(hidden_states, indices, scores)

        # 3. Expert è®¡ç®— (FluidGroupedMLP with dW scheduling)
        expert_output, _ = self.experts(dispatched_input, tokens_per_expert, probs)

        # 4. Token Combine (FluidTokenDispatcher ç›´æ¥è°ƒç”¨ fluid_all_to_all)
        output = self.token_dispatcher.combine(expert_output)

        return output, None
```

### 3. FluidTokenDispatcher (è‡ªå®šä¹‰ Token è·¯ç”±)

```python
class FluidTokenDispatcher:
    """
    è‡ªå®šä¹‰ Token Dispatcher
    - ç›´æ¥è°ƒç”¨ fluid_all_to_all (æ— éœ€ patch Megatron å‡½æ•°)
    - æ”¯æŒå‰å‘ä¼˜åŒ– (router-dispatch overlap ç­‰)
    """

    def dispatch(self, hidden_states, routing_map, probs):
        # Permute tokens
        permuted_tokens, self.permutation_map = permute(hidden_states, routing_map)

        # AllToAll Dispatch (ç›´æ¥è°ƒç”¨,æ— éœ€ patch!)
        if self.ep_size > 1:
            global_tokens = fluid_all_to_all(
                self.ep_group,
                permuted_tokens,
                output_splits,
                input_splits,
                comm_type="moe_dispatch",
            )

        return global_tokens, tokens_per_expert, probs
```

## æ”¯æŒçš„æ¨¡å‹

FluidMoE æ”¯æŒæ‰€æœ‰ Megatron-LM çš„ MoE æ¨¡å‹ï¼š

- âœ… **Mixtral-8x7B** (Mistral AI)
- âœ… **DeepSeekMoE** (DeepSeek)
- âœ… **è‡ªå®šä¹‰ MoE æ¨¡å‹** (åŸºäº Megatron GPTModel)

## ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1: åŸºæœ¬ç”¨æ³•

```python
from fluid import get_fluid_moe_layer_spec

# è·å– layer spec
layer_spec = get_fluid_moe_layer_spec()

# ä½¿ç”¨ layer spec åˆ›å»ºæ¨¡å‹
model = GPTModel(config, transformer_layer_spec=layer_spec)
```

### ç¤ºä¾‹ 2: è‡ªå®šä¹‰å‰å‘è®¡ç®—

```python
from fluid import FluidSelfAttention, FluidMoELayer

# ç»§æ‰¿ FluidSelfAttention å¹¶è‡ªå®šä¹‰ forward
class MyCustomAttention(FluidSelfAttention):
    def forward(self, hidden_states, attention_mask):
        # æ·»åŠ ä½ çš„è‡ªå®šä¹‰é€»è¾‘
        # ä¾‹å¦‚: Ring Attention, è®¡ç®—-é€šä¿¡é‡å ç­‰
        ...
        return super().forward(hidden_states, attention_mask)

# ä½¿ç”¨è‡ªå®šä¹‰å±‚
from megatron.core.transformer.spec_utils import ModuleSpec
layer_spec = ModuleSpec(
    module=TransformerLayer,
    submodules={
        'self_attention': ModuleSpec(module=MyCustomAttention),
        'moe': ModuleSpec(module=FluidMoELayer),
    }
)
```

### ç¤ºä¾‹ 3: æŸ¥çœ‹ Fluid å±‚ä¿¡æ¯

```python
from fluid import print_fluid_layer_info, print_status

# æ‰“å°æ¨¡å‹ä¸­çš„ Fluid å±‚
print_fluid_layer_info(model)

# æ‰“å° FluidMoE çŠ¶æ€
print_status()
```

è¾“å‡º:
```
============================================================
FluidMoE Layer Information
============================================================
Attention layers: 32 FluidSelfAttention
  - decoder.layers.0.self_attention
  - decoder.layers.1.self_attention
  ...

MoE layers: 32 FluidMoELayer
  - decoder.layers.0.moe
  - decoder.layers.1.moe
  ...

Scheduler status: âœ… Enabled
============================================================
```

## ä¼˜åŒ–åŸç†

### Backward dW å»¶è¿Ÿä¸é‡å 

```
æ ‡å‡† Megatron (é¡ºåºæ‰§è¡Œ):
GPU:  [dX + dW] â†’ [AllToAll] â† GPU ç©ºé—² âŒ
Time: â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤
      T_dX+dW    T_AllToAll

FluidMoE (è®¡ç®—é€šä¿¡é‡å ):
GPU:  [dX] â†’ [AllToAll] â† åŒæ—¶æ‰§è¡Œ dW âœ…
           â¬‡
      [dW å¹¶è¡Œæ‰§è¡Œ]
Time: â”œâ”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤
      T_dX  max(T_dW, T_AllToAll)

åŠ é€Ÿæ¯”: (T_dX + T_dW + T_AllToAll) / (T_dX + max(T_dW, T_AllToAll))
```

**èŠ‚çœæ—¶é—´**: å¦‚æœ `T_dW â‰ˆ T_AllToAll`ï¼Œç†è®ºåŠ é€Ÿ ~1.3-1.5x

### å…³é”®è®¾è®¡

1. âœ… **dX ç«‹å³è®¡ç®—** - ä¿è¯æ¢¯åº¦ä¼ æ’­ä¸é˜»å¡
2. âœ… **dW å»¶è¿Ÿæ³¨å†Œ** - æ³¨å†Œåˆ°è°ƒåº¦å™¨é˜Ÿåˆ—
3. âœ… **AllToAll è§¦å‘** - åå‘ä¼ æ’­æ—¶æ‰§è¡Œé˜Ÿåˆ—ä¸­çš„ dW
4. âœ… **GPU ä¸ç©ºé—²** - dW è®¡ç®—å¡«è¡¥ AllToAll ç­‰å¾…æ—¶é—´

## æ€§èƒ½ç»Ÿè®¡

```python
from fluid import get_backward_scheduler

scheduler = get_backward_scheduler()
stats = scheduler.get_stats()

print(f"Total dW tasks: {stats['total_dw_tasks']}")
print(f"Completed dW tasks: {stats['completed_dw_tasks']}")
print(f"Overlap efficiency: {stats['completed_dw_tasks']/stats['total_dw_tasks']*100:.1f}%")
```

**ç†æƒ³é‡å ç‡**:
- **90%+** - ä¼˜ç§€ï¼dW å‡ ä¹å®Œå…¨ä¸ AllToAll é‡å 
- **70-90%** - è‰¯å¥½,éƒ¨åˆ† dW åœ¨é€šä¿¡æœŸé—´å®Œæˆ
- **<70%** - éœ€è¦è°ƒä¼˜,å¯èƒ½ dW å¤ªå¿«æˆ–é€šä¿¡å¤ªæ…¢

## æ–‡æ¡£

| æ–‡æ¡£ | è¯´æ˜ |
|------|------|
| [README.md](README.md) | é¡¹ç›®æ¦‚è§ˆ (æœ¬æ–‡æ¡£) |
| [QUICKSTART.md](QUICKSTART.md) | å¿«é€Ÿå…¥é—¨å’Œä½¿ç”¨ç¤ºä¾‹ |
| [CORE_CONCEPT.md](CORE_CONCEPT.md) | æ ¸å¿ƒæ¦‚å¿µå’Œè®¾è®¡æ€æƒ³ |
| [MAINTENANCE_GUIDE.md](MAINTENANCE_GUIDE.md) | ç»´æŠ¤æŒ‡å—å’Œ Megatron API è·Ÿéšç­–ç•¥ |
| [WORKFLOW.md](WORKFLOW.md) | è¯¦ç»†çš„ä»£ç å·¥ä½œæµç¨‹ |

## è¦æ±‚

- Python >= 3.8
- PyTorch >= 2.0
- Megatron-Core >= 0.5.0
- CUDA >= 11.8

## å®‰è£…

```bash
# å…‹éš†ä»“åº“
cd /path/to/your/project

# å°† Fluid æ·»åŠ åˆ° PYTHONPATH
export PYTHONPATH="/path/to/Fluid:$PYTHONPATH"

# ç¡®ä¿ Megatron-LM ä¹Ÿåœ¨ PYTHONPATH ä¸­
export PYTHONPATH="/path/to/Megatron-LM:$PYTHONPATH"
```

## å®Œæ•´ç¤ºä¾‹

æŸ¥çœ‹ [examples/pretrain_moe.py](examples/pretrain_moe.py) è·å–å®Œæ•´çš„è®­ç»ƒç¤ºä¾‹ã€‚

è¿è¡Œ Mixtral-8x7B:
```bash
bash examples/run_mixtral_8x7b.sh
```

## ä¸ Megatron çš„å…³ç³»

| ç»„ä»¶ | Megatron | FluidMoE |
|------|----------|---------|
| **TransformerLayer** | âœ… ä½¿ç”¨åŸç‰ˆ | - |
| **SelfAttention** | âŒ æ›¿æ¢ | âœ… FluidSelfAttention |
| **MoELayer** | âŒ æ›¿æ¢ | âœ… FluidMoELayer |
| **Router** | âœ… ä½¿ç”¨åŸç‰ˆ | - |
| **DotProductAttention** | âœ… ä½¿ç”¨åŸç‰ˆ | - |
| **TokenDispatcher** | âŒ æ›¿æ¢ | âœ… FluidTokenDispatcher |
| **GroupedMLP** | âŒ æ›¿æ¢ | âœ… FluidGroupedMLP |
| **Linear å±‚** | âŒ æ›¿æ¢ | âœ… Fluid*ParallelLinear |
| **AllToAll å‡½æ•°** | âœ… ä¸ patch | âœ… ç›´æ¥è°ƒç”¨ Fluid ç‰ˆæœ¬ |

**è®¾è®¡åŸåˆ™**:
- æ›¿æ¢æœ€å°‘çš„æ¨¡å— (åªæ›¿æ¢éœ€è¦ä¼˜åŒ–çš„å±‚)
- ç›´æ¥è°ƒç”¨ Fluid AllToAll (ä¸æ±¡æŸ“å…¨å±€å‘½åç©ºé—´)
- å°½å¯èƒ½å¤ç”¨ Megatron ç»„ä»¶ (Router, CoreAttention ç­‰)

## ç»´æŠ¤

FluidMoE ä½¿ç”¨å®Œå…¨è‡ªå®šä¹‰å±‚ï¼Œéœ€è¦è·Ÿéš Megatron API å˜åŒ–ï¼š

```bash
# Megatron æ›´æ–°å
cd Megatron-LM
git pull origin main

# æ£€æŸ¥ SelfAttention å’Œ MoELayer API å˜åŒ–
git diff <old_commit> <new_commit> -- megatron/core/transformer/attention.py
git diff <old_commit> <new_commit> -- megatron/core/transformer/moe/

# åŒæ­¥åˆ° FluidSelfAttention å’Œ FluidMoELayer
cd Fluid
vim fluid/attention_module.py
vim fluid/moe_module.py

# è¿è¡Œæµ‹è¯•
python -m pytest tests/
```

è¯¦è§ [MAINTENANCE_GUIDE.md](MAINTENANCE_GUIDE.md)ã€‚

## ç‰ˆæœ¬å†å²

- **v0.5.0** (å½“å‰): å®Œå…¨è‡ªå®šä¹‰å±‚å®ç°ï¼Œæ— éœ€ patch
- **v0.4.0**: Layer Spec æ¨¡å¼ï¼Œéœ€è¦ patch AllToAll
- **v0.3.0**: Monkey Patching + Layer Spec åŒæ¨¡å¼
- **v0.2.0**: åˆå§‹ Monkey Patching å®ç°

## License

Apache 2.0

## Citation

å¦‚æœ FluidMoE å¯¹ä½ çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨:

```bibtex
@software{fluidmoe2024,
  title={FluidMoE: Complete Custom Layer Implementation for Megatron-LM MoE},
  author={FluidMoE Team},
  year={2024},
  url={https://github.com/your-org/FluidMoE}
}
```
